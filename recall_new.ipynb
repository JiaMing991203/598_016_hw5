{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "ccUpV-W0irX7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "xOEvxLq2rJ4Z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content\")\n",
        "if os.path.exists(\"598_016_hw5\"):\n",
        "    !rm -rf 598_016_hw5\n",
        "!git clone https://github.com/JiaMing991203/598_016_hw5.git\n",
        "os.chdir(\"598_016_hw5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WK39xesrit-d",
        "outputId": "d799604a-c4dc-413c-974b-943e3b786984"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '598_016_hw5'...\n",
            "remote: Enumerating objects: 52, done.\u001b[K\n",
            "remote: Counting objects: 100% (52/52), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 52 (delta 27), reused 43 (delta 18), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (52/52), 22.50 KiB | 500.00 KiB/s, done.\n",
            "Resolving deltas: 100% (27/27), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6SmqABlGjNbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "zO7j1fLc9I56"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "from mamba_model import MambaTwo, MambaConfig\n",
        "from atten_model import BaseNet, AttnRope"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHtXjiRq9I58"
      },
      "source": [
        "We are giving you a 2 layer transformer model in induction task. Check the data generation mechanism for the induction head and adjust it accordingly to your needs. For AR, you need to code one from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "PFDafpbU9I59"
      },
      "outputs": [],
      "source": [
        "class InductionAR(Dataset):\n",
        "    # In induction head we have ngram = 1. But the code provided is for general ngram setting. While using this, initialize ngram = 1.\n",
        "    \"\"\" Naive associative recall dataset \"\"\"\n",
        "    def __init__(self, num_examples, tokenizer, n_gram=1, n_ctx = 1024,\n",
        "                 seed = 0, train_split=0.8, tau: int = 1, ):\n",
        "        self.num_examples = num_examples\n",
        "        self.tokenizer = tokenizer\n",
        "        self.n_ctx = n_ctx\n",
        "        self.seed = seed\n",
        "        self.n_gram = n_gram\n",
        "        self.tau = tau\n",
        "        x, y = self.data_gen()\n",
        "        if train_split:\n",
        "            self.train_x, self.train_y, self.test_x, self.test_y = self.split(x, y, train_split)\n",
        "            self.train = self.numpy_to_tensor_dataset(self.train_x, self.train_y)\n",
        "            self.test = self.numpy_to_tensor_dataset(self.test_x, self.test_y)\n",
        "        else:\n",
        "            self.train_x, self.train_y, self.test_x, self.test_y = x, y, None, None\n",
        "            self.train = self.numpy_to_tensor_dataset(self.train_x, self.train_y)\n",
        "            self.test = None\n",
        "    def get_str_dataset(self, split=\"train\"):\n",
        "        if split == \"train\":\n",
        "            x_str = [self.tokenizer.decode(xi) for xi in self.train_x]\n",
        "            y_str = [self.tokenizer.decode([yi]) for yi in self.train_y]\n",
        "        elif split == \"test\":\n",
        "            x_str = [self.tokenizer.decode(xi) for xi in self.test_x]\n",
        "            y_str = [self.tokenizer.decode([yi]) for yi in self.test_y]\n",
        "        else:\n",
        "            raise ValueError(\"split should be either 'train' or 'test'\")\n",
        "        return x_str, y_str\n",
        "    def numpy_to_tensor_dataset(self, x, y):\n",
        "        x = torch.tensor(x, dtype=torch.long)\n",
        "        y = torch.tensor(y, dtype=torch.long)\n",
        "        return TensorDataset(x, y)\n",
        "    def gen_single_example(self):\n",
        "        # get the vocab size\n",
        "        def count(str_x, str_n_gram_head):\n",
        "            counts = sum([\n",
        "                str_x.startswith(str_n_gram_head, i) for i in range(len(str_x))\n",
        "            ])\n",
        "            return counts\n",
        "        def gen_x():\n",
        "            gen_x_success = False\n",
        "            while not gen_x_success:\n",
        "                x = np.random.choice(vocab, self.n_ctx-self.n_gram*2, replace=True).tolist()\n",
        "                # remove the case where the n_gram_head is repeated in the sequence\n",
        "                for _ in range(10):\n",
        "                    pos = [i for i in range(len(x)-len(n_gram_head)+1) if x[i:i+len(n_gram_head)] == n_gram_head]\n",
        "                    if len(pos) == 0:\n",
        "                        gen_x_success = True\n",
        "                        break\n",
        "                    else:\n",
        "                        # remove the n_gram_head from x\n",
        "                        # get all positions of the n_gram_head in x\n",
        "                        for p in reversed(pos):\n",
        "                            # remove len(n_gram_head) elements from x starting from p\n",
        "                            x = x[:p] + x[p+len(n_gram_head):]\n",
        "                        # fill the rest of the sequence with random elements\n",
        "                        x.extend(np.random.choice(vocab, self.n_ctx-self.n_gram*2-len(x), replace=True).tolist())\n",
        "                x_test = \" \".join([str(xi) for xi in x])\n",
        "                if count(x_test, str_n_gram_head) == 0:\n",
        "                    gen_x_success = True\n",
        "\n",
        "            x_test = x + n_gram_head\n",
        "            # check if there's only one n_gram_head in the sequence\n",
        "            # to avoid the case where the n_gram_head has\n",
        "            # repeated structure such as x= [1, 2, 3, 1] , n_gram_head = [1, 1]\n",
        "            str_x_test = \" \"+\" \".join([str(xi) for xi in x_test])+ \" \"\n",
        "            if count(str_x_test, str_n_gram_head) > 1:\n",
        "                print(\"Error in gen_x\")\n",
        "                print(f\"str_x_test: {str_x_test}\", f\"str_n_gram_head: {str_n_gram_head}\",\n",
        "                      \"count: \", count(str_x_test, str_n_gram_head))\n",
        "            if count(str_x_test, str_n_gram_head) == 1:\n",
        "                return x\n",
        "            else:\n",
        "                return None\n",
        "        def insert_n_gram_head(x):\n",
        "            pos = random.randint(0, len(x)-self.tau)\n",
        "            y = x[pos + self.tau - 1]\n",
        "            x_new = x[:pos] + n_gram_head + x[pos:] + n_gram_head\n",
        "            str_x_new = \" \"+\" \".join([str(xi) for xi in x_new])+\" \"\n",
        "\n",
        "            if count(str_x_new, str_n_gram_head) == 2:\n",
        "                return x_new, y\n",
        "            else:\n",
        "                return None, None\n",
        "        vocab_size = len(self.tokenizer)\n",
        "        vocab = list(range(vocab_size))\n",
        "        # set a deterministic n_gram_head\n",
        "        n_gram_head = list(range(self.n_gram))\n",
        "\n",
        "        str_n_gram_head = \" \"+\" \".join([str(xi) for xi in n_gram_head])+\" \"\n",
        "        assert self.n_gram*2 < self.n_ctx, \"n_gram*2 should be less than n_ctx\"\n",
        "        success = False\n",
        "        while not success:\n",
        "            x = gen_x()\n",
        "            if x is not None:\n",
        "                for _ in range(10):\n",
        "                    x_new, y = insert_n_gram_head(x)\n",
        "                    if x_new is not None:\n",
        "                        success = True\n",
        "                        break\n",
        "        return x_new, y\n",
        "\n",
        "    def data_gen(self):\n",
        "        x = []\n",
        "        y = []\n",
        "        # get previous random status and recover after generating the dataset\n",
        "        random_status = random.getstate()\n",
        "        random.seed(self.seed)\n",
        "        for i in range(self.num_examples):\n",
        "            if i % 1000 == 0:\n",
        "                print(f\"Generating example {i}\")\n",
        "            xi, yi = self.gen_single_example()\n",
        "            x.append(xi)\n",
        "            y.append(yi)\n",
        "        x = np.array(x)\n",
        "        y = np.array(y)\n",
        "        random.setstate(random_status)\n",
        "        return x, y\n",
        "    def split(self, x, y, train_ratio = 0.8):\n",
        "        num_train = int(len(x)*train_ratio)\n",
        "        train_x = x[:num_train]\n",
        "        train_y = y[:num_train]\n",
        "        test_x = x[num_train:]\n",
        "        test_y = y[num_train:]\n",
        "        return train_x, train_y, test_x, test_y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "4I1owAoh9I5-"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Random_tokenizer:\n",
        "    def __init__(self, vocab=None, vocab_size = None) -> None:\n",
        "        \"\"\" The init function of the tokenizer class.\n",
        "         one of vocab or vocab_size should be provided.\n",
        "         If vocab is provided, vocab_size will be ignored.\n",
        "         If vocab is not provided, vocab_size should be provided. we will generate a random vocab of vocab_size.\"\"\"\n",
        "        if vocab is not None:\n",
        "            self.vocab = vocab\n",
        "            self.vocab_size = len(vocab)\n",
        "        elif vocab_size is not None:\n",
        "            self.vocab_size = vocab_size\n",
        "            self.vocab = [str(i) for i in range(vocab_size)]\n",
        "        else:\n",
        "            raise ValueError(\"one of vocab or vocab_size should be provided.\")\n",
        "        self.vocab_dict = {v: i for i, v in  enumerate(self.vocab)}\n",
        "        self.vocab_dict_inv = {i: v for i, v in enumerate(self.vocab)}\n",
        "    def encode(self, x):\n",
        "        \"\"\" Encode a string into a list of integers \"\"\"\n",
        "        return [self.vocab_dict[i] for i in x]\n",
        "    def decode(self, x):\n",
        "        \"\"\" Decode a list of integers into a string \"\"\"\n",
        "        return ' '.join([self.vocab_dict_inv[i] for i in x])\n",
        "    def __len__(self):\n",
        "        return self.vocab_size\n",
        "    def __getitem__(self, i):\n",
        "        return self.vocab[i]\n",
        "    def __iter__(self):\n",
        "        return iter(self.vocab)\n",
        "    def __contains__(self, x):\n",
        "        return x in self.vocab\n",
        "    def __repr__(self):\n",
        "        return f\"Random_tokenizer(vocab_size={self.vocab_size})\"\n",
        "    def __str__(self):\n",
        "        return f\"Random_tokenizer(vocab_size={self.vocab_size})\"\n",
        "    def __call__(self, x):\n",
        "        return self.encode(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "zZgyxBsU9I5-"
      },
      "outputs": [],
      "source": [
        "# self attention block\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, embed_dim, max_len=11):\n",
        "        super(Block, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.max_len = max_len\n",
        "        self.c_attn = nn.Linear(embed_dim, embed_dim*3)\n",
        "        self.register_buffer('mask', torch.tril(torch.ones(max_len, max_len)))\n",
        "    def forward(self, x):\n",
        "        T = x.size(1)\n",
        "        q, k, v = self.c_attn(x).chunk(3, dim=-1)\n",
        "        y = torch.nn.functional.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7WoFgGH9I5-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using {device} device\")\n",
        "seed = 0\n",
        "n_ctx = 10 # training sequence length\n",
        "num_examples = 100 # generate 100000 examples\n",
        "batch_size = 11 # batch size\n",
        "vocab_size = 10 # vocabulary size\n",
        "num_epochs = 100    # number of epochs\n",
        "attn_layers = 2 # number of attention layers\n",
        "embed_dim = 8 # embedding dimension\n",
        "is_pe = True  # the default positional embedding we are using is the learned positional embedding\n",
        "\n",
        "\n",
        "tokenizer = Random_tokenizer(vocab_size=vocab_size)\n",
        "dataset = InductionAR(num_examples, tokenizer, 1, n_ctx=n_ctx, seed=seed, train_split=0.8, tau=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-r6N63nm9UBQ",
        "outputId": "12b62a91-ab54-4cc6-a355-96cec4483d97"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "Generating example 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QukGmTn492Rm",
        "outputId": "1d751830-768a-424c-dada-a7a4b279695b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.InductionAR at 0x7ea7eaf85d80>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_mamba_two(embed_dim, )"
      ],
      "metadata": {
        "id": "GgxFzvz6JYeg"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "mamba_config = MambaConfig(d_model = embed_dim)\n",
        "\n",
        "# mamba_config.d_state = vocab_size"
      ],
      "metadata": {
        "id": "zt6a-jSoJJZW"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "I44U1-C19I5-",
        "outputId": "c2b3ec9e-503e-4196-b83d-0032ac5c64b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "after embed shape: torch.Size([11, 10, 8])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (10) must match the size of tensor b (40) at non-singleton dimension 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-de334a5ca077>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/598_016_hw5/atten_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after embed shape: {x.size()}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpe_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mln\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpe_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after pe shape: {x.size()}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/598_016_hw5/atten_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;31m# Add the positional embedding to the input tensor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional_embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m# Apply the rotation matrix to the input tensor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (40) at non-singleton dimension 1"
          ]
        }
      ],
      "source": [
        "\n",
        "train_loader = DataLoader(dataset.train, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset.test, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# attn_model = BaseNet(len(tokenizer), embed_dim, is_pe,  max_len=n_ctx*4, attn_layers=attn_layers, block=Block).to(device)\n",
        "mamba_model = MambaTwo(mamba_config, vocab_size)\n",
        "\n",
        "rope_model = AttnRope(vocab_size, embed_dim, max_len=n_ctx*4, attn_layers=2, block=Block).to(device)\n",
        "\n",
        "\n",
        "models = [rope_model] # add more models here when you have more models\n",
        "for model in models:\n",
        "    model.train()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(x)[:,-1]\n",
        "            loss = criterion(y_pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"epoch {epoch} loss: {total_loss/len(train_loader)}\")\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in test_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            y_pred = model(x)[:,-1]\n",
        "            y_pred = F.softmax(y_pred, dim=-1)\n",
        "            y_pred = torch.argmax(y_pred, dim=-1)\n",
        "            correct += (y_pred == y).sum().item()\n",
        "            total += y.size(0)\n",
        "    print(f\"Test accuracy: {correct/total}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "HUfFvqMO9I5_",
        "outputId": "9433aa2d-e5fe-42e5-b0f9-7db48922be64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating example 0\n",
            "Generating example 1000\n",
            "Generating example 2000\n",
            "Generating example 3000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-a6ebb821dba8>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mn_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInductionAR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_ctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-75-381a8574ad50>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_examples, tokenizer, n_gram, n_ctx, seed, train_split, tau)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_gram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtau\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain_split\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-75-381a8574ad50>\u001b[0m in \u001b[0;36mdata_gen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Generating example {i}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mxi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen_single_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-75-381a8574ad50>\u001b[0m in \u001b[0;36mgen_single_example\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0msuccess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_x\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-75-381a8574ad50>\u001b[0m in \u001b[0;36mgen_x\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mgen_x_success\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgen_x_success\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_ctx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gram\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                 \u001b[0;31m# remove the case where the n_gram_head is repeated in the sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# generate test data with length 32\n",
        "# test the model with the test data\n",
        "num_examples = 20000\n",
        "n_ctx = 32\n",
        "\n",
        "dataset = InductionAR(num_examples, tokenizer, 1, n_ctx=n_ctx, seed=seed, train_split=0.01)\n",
        "train_loader = DataLoader(dataset.train, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset.test, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "attn_model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        x,y = x.to(device), y.to(device)\n",
        "        y_pred = attn_model(x)[:,-1]\n",
        "        y_pred = F.softmax(y_pred, dim=-1)\n",
        "        y_pred = torch.argmax(y_pred, dim=-1)\n",
        "        correct += (y_pred == y).sum().item()\n",
        "        total += y.size(0)\n",
        "    print(f\"Test accuracy: {correct/total}\")\n",
        "\n",
        "# generate test data with length 16\n",
        "# test the model with the test data\n",
        "num_examples = 20000\n",
        "n_ctx = 16\n",
        "\n",
        "dataset = InductionAR(num_examples, tokenizer, 1, n_ctx=n_ctx, seed=seed, train_split=0.01)\n",
        "train_loader = DataLoader(dataset.train, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset.test, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "attn_model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        x,y = x.to(device), y.to(device)\n",
        "        y_pred = attn_model(x)[:,-1]\n",
        "        y_pred = F.softmax(y_pred, dim=-1)\n",
        "        y_pred = torch.argmax(y_pred, dim=-1)\n",
        "        correct += (y_pred == y).sum().item()\n",
        "        total += y.size(0)\n",
        "    print(f\"Test accuracy: {correct/total}\")\n",
        "\n",
        "# generate test data with length 128\n",
        "# test the model with the test data\n",
        "num_examples = 20000\n",
        "n_ctx = 128\n",
        "\n",
        "dataset = InductionAR(num_examples, tokenizer, 1, n_ctx=n_ctx, seed=seed, train_split=0.01)\n",
        "train_loader = DataLoader(dataset.train, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset.test, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "attn_model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        x,y = x.to(device), y.to(device)\n",
        "        y_pred = attn_model(x)[:,-1]\n",
        "        y_pred = F.softmax(y_pred, dim=-1)\n",
        "        y_pred = torch.argmax(y_pred, dim=-1)\n",
        "        correct += (y_pred == y).sum().item()\n",
        "        total += y.size(0)\n",
        "    print(f\"Test accuracy: {correct/total}\")\n",
        "\n",
        "\n",
        "# generate test data with length 256\n",
        "# test the model with the test data\n",
        "num_examples = 20000\n",
        "n_ctx = 256\n",
        "\n",
        "dataset = InductionAR(num_examples, tokenizer, 1, n_ctx=n_ctx, seed=seed, train_split=0.01)\n",
        "train_loader = DataLoader(dataset.train, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset.test, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "attn_model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        x,y = x.to(device), y.to(device)\n",
        "        y_pred = attn_model(x)[:,-1]\n",
        "        y_pred = F.softmax(y_pred, dim=-1)\n",
        "        y_pred = torch.argmax(y_pred, dim=-1)\n",
        "        correct += (y_pred == y).sum().item()\n",
        "        total += y.size(0)\n",
        "    print(f\"Test accuracy: {correct/total}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "itransformer",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}